TD(0) Learning (Q-Learning) for FrozenLake

1. Introduction to TD(0) Learning
   Temporal Difference (TD) learning is a central idea in Reinforcement Learning (RL). It combines ideas from Monte Carlo methods and Dynamic Programming (DP). 
   - Like Monte Carlo, TD methods can learn directly from raw experience without a model of the environment's dynamics.
   - Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap).

   "TD(0)" typically refers to the one-step TD prediction method for estimating state values. However, for control (training an agent to take actions), we use Q-Learning, which is an off-policy TD control algorithm.

2. Why Q-Learning for FrozenLake?
   FrozenLake is a grid-world environment where the agent must navigate from Start (S) to Goal (G) on a frozen lake with holes (H). The ice is slippery, meaning the agent might not move in the intended direction.
   - Model-Free: We don't need to know the transition probabilities (how slippery the ice is) beforehand. The agent learns by trying actions.
   - Online Learning: The agent learns while interacting with the environment step-by-step.
   - Off-Policy: Q-Learning learns the optimal policy (target policy) while acting using an exploratory policy (behavior policy, e.g., epsilon-greedy). This is useful for balancing exploration and exploitation.

3. The Q-Learning Update
   The core of the algorithm is the update rule for the Q-value Q(s, a):
   
   Q(s, a) <- Q(s, a) + alpha * [R + gamma * max(Q(s', a')) - Q(s, a)]

   where:
   - s: Current state
   - a: Action taken
   - R: Reward received
   - s': Next state
   - alpha: Learning rate (how much we accept new information)
   - gamma: Discount factor (importance of future rewards)
   - max(Q(s', a')): Estimated best future value (bootstrapping)

4. Training Process
   - The agent explores the lake using an epsilon-greedy strategy (mostly random at first, then more greedy).
   - When it falls into a hole or reaches the goal, it receives a reward (0 or 1) and updates its Q-table.
   - Over thousands of episodes, the Q-values converge to the true expected returns, allowing the agent to navigate safely to the goal even with slippery ice.

5. FrozenLake-v1 Environment Details
   - **Grid**: 4x4 grid (16 states total).
   - **Start (S)**: Top-left corner [0].
   - **Goal (G)**: Bottom-right corner [15].
   - **Holes (H)**: Fatal states to be avoided.
   - **Frozen (F)**: Safe states.
   - **Actions (4)**: 0: LEFT, 1: DOWN, 2: RIGHT, 3: UP.
   - **Rewards**: +1 for reaching the Goal (G), 0 otherwise.
   - **Slippery**: The environment is stochastic. Moving in a direction has a 1/3 probability of moving in the intended direction, and 1/3 each for moving in perpendicular directions. This makes the "safe" path non-trivial.

6. Evaluation Metrics
   - **Success Rate (Mean Reward)**: Since the reward is binary (1 for success, 0 for failure), the Average Reward over N episodes is exactly the Success Rate.
     - A random agent typically achieves ~1.5% success rate.
     - A trained Q-Learning agent typically achieves >20% to ~70% success rate, depending on the slipperiness and training duration. (Theoretical max is <100% due to un-recoverable slippery transitions).
   - **Q-Value Convergence**: We monitor how the Q-values stabilize. In the training plot, we look for the rolling average reward to increase and plateau.

7. Code-Level Differences: TD(0) vs. Monte Carlo
   The implementation of Training Code differs significantly between TD(0) (Q-Learning) and Monte Carlo (MC) methods:

   A. When Updates Happen
      - **TD(0)/Q-Learning** updates the Q-table **immediately after every step**.
        - *Code structure*: Inside the `while not done:` loop, you calculate the `td_error` and update `q_table[state, action]` right away using `reward` and `next_state`.
      - **Monte Carlo** updates the Q-table only **at the end of the episode**.
        - *Code structure*: Inside the `while not done:` loop, you just store the transition `(state, action, reward)` in a list (memory). After the loop finishes, you iterate backwards through this list to calculate the total return G and update the Q-table.

   B. Bootstrapping vs. Actual Return
      - **TD(0)** uses **Bootstrapping**: The update target `R + gamma * max(Q(s', a'))` uses the *current estimated value* of the next state. It guesses based on a guess.
      - **Monte Carlo** uses the **Actual Return**: The update target `G` is the actual sum of discounted rewards received from that point until the end of the episode. It relies on the final outcome.

   C. Dependence on Episode Completion
      - **TD(0)** does **not** need the episode to finish to learn. It learns online. This is crucial for continuous tasks or very long episodes.
      - **Monte Carlo** **requires** the episode to terminate before it can learn anything, as it needs the final return.


